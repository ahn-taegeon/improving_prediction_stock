{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOZQ3hKkK3ok5sK3JR4EyLp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","\n","data = pd.read_csv(\"total.csv\")\n","selected_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'N_Sentiment', 'C_Sentiment', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 열 이름들\n","target_column = 'target'\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","# 데이터 준비\n","X = data[selected_features].values\n","y = data[target_column].values\n","\n","# 훈련/테스트 데이터 분리\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 데이터 표준화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀\n","logistic_model = LogisticRegression()\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","\n","# 3. XGBoost\n","xgb_model = XGBClassifier(random_state=42)\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","\n","# 평가 지표\n","print(\"\\nClassification Report (XGBoost):\")\n","print(classification_report(y_test, xgb_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTR8V28ziLp1","executionInfo":{"status":"ok","timestamp":1732089215411,"user_tz":-540,"elapsed":290,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"d91737e4-6272-401e-b040-8e955e092b56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy: 0.7346938775510204\n","Random Forest Accuracy: 0.7551020408163265\n","XGBoost Accuracy: 0.7142857142857143\n","\n","Classification Report (XGBoost):\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.40      0.50         5\n","           1       0.79      0.83      0.81        36\n","           2       0.38      0.38      0.38         8\n","\n","    accuracy                           0.71        49\n","   macro avg       0.61      0.54      0.56        49\n","weighted avg       0.71      0.71      0.71        49\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","\n","data = pd.read_csv(\"total.csv\")\n","selected_features = ['Open', 'Close', 'Volume', 'N_Sentiment', 'C_Sentiment', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 열 이름들\n","target_column = 'target'\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","# 데이터 준비\n","X = data[selected_features].values\n","y = data[target_column].values\n","\n","# 훈련/테스트 데이터 분리\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 데이터 표준화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀\n","logistic_model = LogisticRegression()\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","\n","# 3. XGBoost\n","xgb_model = XGBClassifier(random_state=42)\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","\n","# 평가 지표\n","print(\"\\nClassification Report (XGBoost):\")\n","print(classification_report(y_test, xgb_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGrWz88nkFAR","executionInfo":{"status":"ok","timestamp":1732089253806,"user_tz":-540,"elapsed":615,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"cbea9a8a-4f9b-490a-9ccd-7e2a25554aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy: 0.7346938775510204\n","Random Forest Accuracy: 0.7142857142857143\n","XGBoost Accuracy: 0.7551020408163265\n","\n","Classification Report (XGBoost):\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.40      0.44         5\n","           1       0.80      0.89      0.84        36\n","           2       0.60      0.38      0.46         8\n","\n","    accuracy                           0.76        49\n","   macro avg       0.63      0.55      0.58        49\n","weighted avg       0.74      0.76      0.74        49\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","import pandas as pd\n","\n","data = pd.read_csv(\"total.csv\")\n","selected_features = ['Open','High', 'Low', 'Close', 'Volume', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 열 이름들\n","target_column = 'target'\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","# 데이터 준비\n","X = data[selected_features].values\n","y = data[target_column].values\n","\n","# 훈련/테스트 데이터 분리\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 데이터 표준화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀\n","logistic_model = LogisticRegression()\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","\n","# 3. XGBoost\n","xgb_model = XGBClassifier(random_state=42)\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","\n","# 평가 지표\n","print(\"\\nClassification Report (XGBoost):\")\n","print(classification_report(y_test, xgb_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zuEHm2TlaYc","executionInfo":{"status":"ok","timestamp":1732089633681,"user_tz":-540,"elapsed":580,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"0313092f-5e06-4c56-c842-ce0e1fce20b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy: 0.7142857142857143\n","Random Forest Accuracy: 0.7346938775510204\n","XGBoost Accuracy: 0.7142857142857143\n","\n","Classification Report (XGBoost):\n","              precision    recall  f1-score   support\n","\n","           0       0.33      0.20      0.25         5\n","           1       0.79      0.86      0.83        36\n","           2       0.43      0.38      0.40         8\n","\n","    accuracy                           0.71        49\n","   macro avg       0.52      0.48      0.49        49\n","weighted avg       0.69      0.71      0.70        49\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# 데이터 준비\n","# 데이터프레임 `data`를 사용한다고 가정\n","data = pd.read_csv(\"total.csv\")\n","selected_features = ['Open', 'Close', 'Volume', 'N_Sentiment', 'C_Sentiment', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀\n","logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","\n","# 평가\n","print(\"Logistic Regression\")\n","print(\"Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, logistic_preds))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트\n","rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","\n","# 평가\n","print(\"\\nRandom Forest\")\n","print(\"Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, rf_preds))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, rf_preds))\n","\n","# 3. XGBoost\n","xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","\n","# 평가\n","print(\"\\nXGBoost\")\n","print(\"Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, xgb_preds))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, xgb_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2utZEITQoCaj","executionInfo":{"status":"ok","timestamp":1732090552045,"user_tz":-540,"elapsed":1122,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"ab71e072-2510-40c5-d98f-e40a6ae5cf1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression\n","Accuracy: 0.7704918032786885\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.20      0.29         5\n","           1       0.79      0.96      0.87        47\n","           2       0.50      0.11      0.18         9\n","\n","    accuracy                           0.77        61\n","   macro avg       0.60      0.42      0.44        61\n","weighted avg       0.72      0.77      0.72        61\n","\n","Confusion Matrix:\n","[[ 1  4  0]\n"," [ 1 45  1]\n"," [ 0  8  1]]\n","\n","Random Forest\n","Accuracy: 0.7377049180327869\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.91      0.84        47\n","           2       0.67      0.22      0.33         9\n","\n","    accuracy                           0.74        61\n","   macro avg       0.48      0.38      0.39        61\n","weighted avg       0.70      0.74      0.70        61\n","\n","Confusion Matrix:\n","[[ 0  5  0]\n"," [ 3 43  1]\n"," [ 0  7  2]]\n","\n","XGBoost\n","Accuracy: 0.7049180327868853\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.29      0.40      0.33         5\n","           1       0.81      0.81      0.81        47\n","           2       0.43      0.33      0.38         9\n","\n","    accuracy                           0.70        61\n","   macro avg       0.51      0.51      0.51        61\n","weighted avg       0.71      0.70      0.71        61\n","\n","Confusion Matrix:\n","[[ 2  3  0]\n"," [ 5 38  4]\n"," [ 0  6  3]]\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","import pandas as pd\n","from google.colab import drive\n","from sklearn.metrics import f1_score\n","\n","drive.mount('/content/drive')\n","\n","# 데이터 파일 경로 설정\n","file_name1 = '/content/drive/MyDrive/total.csv'\n","data = pd.read_csv(file_name1)\n","selected_features = ['Open', 'Close', 'Volume', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀 최적화\n","logistic_params = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # 규제 강도\n","    'solver': ['lbfgs', 'liblinear', 'sag'],  # 최적화 알고리즘\n","}\n","logistic_model = LogisticRegression(multi_class='multinomial', random_state=42)\n","logistic_search = GridSearchCV(logistic_model, logistic_params, cv=5, scoring='accuracy', n_jobs=-1)\n","logistic_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Logistic Regression):\", logistic_search.best_params_)\n","logistic_best = logistic_search.best_estimator_\n","logistic_preds = logistic_best.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(classification_report(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트 최적화\n","rf_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [5, 10, 20, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","}\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_search = RandomizedSearchCV(rf_model, rf_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","rf_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Random Forest):\", rf_search.best_params_)\n","rf_best = rf_search.best_estimator_\n","rf_preds = rf_best.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(classification_report(y_test, rf_preds))\n","\n","# 3. XGBoost 최적화\n","xgb_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [3, 5, 10],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","}\n","xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)\n","xgb_search = RandomizedSearchCV(xgb_model, xgb_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","xgb_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (XGBoost):\", xgb_search.best_params_)\n","xgb_best = xgb_search.best_estimator_\n","xgb_preds = xgb_best.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(classification_report(y_test, xgb_preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aE8rzv8BqCh0","executionInfo":{"status":"ok","timestamp":1732294511357,"user_tz":-540,"elapsed":7937,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"a3d86d97-25a7-424c-e3f0-f4ce15d87449"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Best Parameters (Logistic Regression): {'C': 100, 'solver': 'lbfgs'}\n","Logistic Regression Accuracy: 0.8688524590163934\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.60      0.55         5\n","           1       0.91      0.91      0.91        47\n","           2       0.88      0.78      0.82         9\n","\n","    accuracy                           0.87        61\n","   macro avg       0.76      0.76      0.76        61\n","weighted avg       0.88      0.87      0.87        61\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n","25 fits failed out of a total of 75.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","25 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1267, in fit\n","    multi_class = _check_multi_class(multi_class, solver, len(self.classes_))\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 99, in _check_multi_class\n","    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n","ValueError: Solver liblinear does not support a multinomial backend.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.72162162        nan 0.72162162 0.7487988         nan 0.7487988\n"," 0.77027027        nan 0.77027027 0.81366366        nan 0.79744745\n"," 0.85225225        nan 0.81396396]\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters (Random Forest): {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None}\n","Random Forest Accuracy: 0.7049180327868853\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.77      0.87      0.82        47\n","           2       0.40      0.22      0.29         9\n","\n","    accuracy                           0.70        61\n","   macro avg       0.39      0.36      0.37        61\n","weighted avg       0.66      0.70      0.67        61\n","\n","Best Parameters (XGBoost): {'subsample': 0.6, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n","XGBoost Accuracy: 0.7377049180327869\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.91      0.84        47\n","           2       0.67      0.22      0.33         9\n","\n","    accuracy                           0.74        61\n","   macro avg       0.48      0.38      0.39        61\n","weighted avg       0.70      0.74      0.70        61\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","import pandas as pd\n","from google.colab import drive\n","from sklearn.metrics import f1_score\n","\n","drive.mount('/content/drive')\n","\n","# 데이터 파일 경로 설정\n","file_name1 = '/content/drive/MyDrive/total.csv'\n","data = pd.read_csv(file_name1)\n","selected_features = ['Open', 'Close', 'Volume', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1:0, 0:1, 1:2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀 최적화\n","logistic_params = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # 규제 강도\n","    'solver': ['lbfgs', 'liblinear', 'sag'],  # 최적화 알고리즘\n","}\n","logistic_model = LogisticRegression(multi_class='multinomial', random_state=42)\n","logistic_search = GridSearchCV(logistic_model, logistic_params, cv=5, scoring='accuracy', n_jobs=-1)\n","logistic_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Logistic Regression):\", logistic_search.best_params_)\n","logistic_best = logistic_search.best_estimator_\n","logistic_preds = logistic_best.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(classification_report(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트 최적화\n","rf_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [5, 10, 20, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","}\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_search = RandomizedSearchCV(rf_model, rf_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","rf_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Random Forest):\", rf_search.best_params_)\n","rf_best = rf_search.best_estimator_\n","rf_preds = rf_best.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(classification_report(y_test, rf_preds))\n","\n","# 3. XGBoost 최적화\n","xgb_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [3, 5, 10],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","}\n","xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)\n","xgb_search = RandomizedSearchCV(xgb_model, xgb_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","xgb_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (XGBoost):\", xgb_search.best_params_)\n","xgb_best = xgb_search.best_estimator_\n","xgb_preds = xgb_best.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(classification_report(y_test, xgb_preds))"],"metadata":{"id":"g804ojUI10AF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# 데이터 파일 경로 설정\n","file_name1 = '/content/drive/MyDrive/total.csv'\n","data = pd.read_csv(file_name1)\n","selected_features = ['Open', 'Close', 'Volume', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1: 0, 0: 1, 1: 2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 미리 설정한 파라미터 튜플\n","tuned_parameters = {\n","    'logistic_regression': {\n","        'C': 100,\n","        'solver': 'lbfgs'\n","    },\n","    'random_forest': {\n","        'n_estimators': 200,\n","        'max_depth': None,\n","        'min_samples_split': 5,\n","        'min_samples_leaf': 2\n","    },\n","    'xgboost': {\n","        'n_estimators': 200,\n","        'max_depth': 10,\n","        'learning_rate': 0.01,\n","        'subsample': 0.6,\n","        'colsample_bytree': 0.6\n","    }\n","}\n","\n","# 1. 로지스틱 회귀 모델 생성 및 평가\n","logistic_model = LogisticRegression(\n","    C=tuned_parameters['logistic_regression']['C'],\n","    solver=tuned_parameters['logistic_regression']['solver'],\n","    multi_class='multinomial',\n","    random_state=42\n",")\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(classification_report(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트 모델 생성 및 평가\n","rf_model = RandomForestClassifier(\n","    n_estimators=tuned_parameters['random_forest']['n_estimators'],\n","    max_depth=tuned_parameters['random_forest']['max_depth'],\n","    min_samples_split=tuned_parameters['random_forest']['min_samples_split'],\n","    min_samples_leaf=tuned_parameters['random_forest']['min_samples_leaf'],\n","    random_state=42\n",")\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(classification_report(y_test, rf_preds))\n","\n","# 3. XGBoost 모델 생성 및 평가\n","xgb_model = XGBClassifier(\n","    n_estimators=tuned_parameters['xgboost']['n_estimators'],\n","    max_depth=tuned_parameters['xgboost']['max_depth'],\n","    learning_rate=tuned_parameters['xgboost']['learning_rate'],\n","    subsample=tuned_parameters['xgboost']['subsample'],\n","    colsample_bytree=tuned_parameters['xgboost']['colsample_bytree'],\n","    objective='multi:softmax',\n","    num_class=len(y.unique()),\n","    random_state=42\n",")\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(classification_report(y_test, xgb_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o7zoMGW24BN","executionInfo":{"status":"ok","timestamp":1732295515994,"user_tz":-540,"elapsed":3222,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"0c7ed9e9-6edd-4d24-c944-dc65b7d9238e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Logistic Regression Accuracy: 0.819672131147541\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.40      0.40         5\n","           1       0.88      0.89      0.88        47\n","           2       0.75      0.67      0.71         9\n","\n","    accuracy                           0.82        61\n","   macro avg       0.67      0.65      0.66        61\n","weighted avg       0.82      0.82      0.82        61\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy: 0.7213114754098361\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.89      0.83        47\n","           2       0.67      0.22      0.33         9\n","\n","    accuracy                           0.72        61\n","   macro avg       0.48      0.37      0.39        61\n","weighted avg       0.70      0.72      0.69        61\n","\n","XGBoost Accuracy: 0.7213114754098361\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.89      0.83        47\n","           2       0.50      0.22      0.31         9\n","\n","    accuracy                           0.72        61\n","   macro avg       0.43      0.37      0.38        61\n","weighted avg       0.67      0.72      0.69        61\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","import pandas as pd\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# 데이터 파일 경로 설정\n","file_name1 = '/content/drive/MyDrive/total.csv'\n","data = pd.read_csv(file_name1)\n","selected_features = ['Open', 'Close', 'Volume', 'N_Sentiment','C_Sentiment','Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1: 0, 0: 1, 1: 2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 1. 로지스틱 회귀 모델 최적화\n","logistic_params = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # 규제 강도\n","    'solver': ['lbfgs', 'liblinear', 'sag'],  # 최적화 알고리즘\n","}\n","logistic_model = LogisticRegression(multi_class='multinomial', random_state=42)\n","logistic_search = GridSearchCV(logistic_model, logistic_params, cv=5, scoring='accuracy', n_jobs=-1)\n","logistic_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Logistic Regression):\", logistic_search.best_params_)\n","logistic_best = logistic_search.best_estimator_\n","logistic_preds = logistic_best.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(classification_report(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트 모델 최적화\n","rf_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [5, 10, 20, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","}\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_search = RandomizedSearchCV(rf_model, rf_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","rf_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (Random Forest):\", rf_search.best_params_)\n","rf_best = rf_search.best_estimator_\n","rf_preds = rf_best.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(classification_report(y_test, rf_preds))\n","\n","# 3. XGBoost 모델 최적화\n","xgb_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [3, 5, 10],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","}\n","xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)\n","xgb_search = RandomizedSearchCV(xgb_model, xgb_params, cv=5, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42)\n","xgb_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (XGBoost):\", xgb_search.best_params_)\n","xgb_best = xgb_search.best_estimator_\n","xgb_preds = xgb_best.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(classification_report(y_test, xgb_preds))\n","\n","# 4. KNN 모델 최적화\n","knn_params = {\n","    'n_neighbors': [3, 5, 7, 9],\n","    'weights': ['uniform', 'distance'],\n","    'metric': ['euclidean', 'manhattan', 'minkowski']\n","}\n","knn_model = KNeighborsClassifier()\n","knn_search = GridSearchCV(knn_model, knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n","knn_search.fit(X_train, y_train)\n","\n","print(\"Best Parameters (KNN):\", knn_search.best_params_)\n","knn_best = knn_search.best_estimator_\n","knn_preds = knn_best.predict(X_test)\n","print(\"KNN Accuracy:\", accuracy_score(y_test, knn_preds))\n","print(classification_report(y_test, knn_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdk6V8u14tJ0","executionInfo":{"status":"ok","timestamp":1732296153533,"user_tz":-540,"elapsed":11070,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"6d018ce7-664e-494e-92b1-9cbd53ea4e07"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n","25 fits failed out of a total of 75.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","25 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1267, in fit\n","    multi_class = _check_multi_class(multi_class, solver, len(self.classes_))\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 99, in _check_multi_class\n","    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n","ValueError: Solver liblinear does not support a multinomial backend.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.72162162        nan 0.72162162 0.7487988         nan 0.7487988\n"," 0.77027027        nan 0.77027027 0.81366366        nan 0.79744745\n"," 0.85225225        nan 0.81396396]\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters (Logistic Regression): {'C': 100, 'solver': 'lbfgs'}\n","Logistic Regression Accuracy: 0.8688524590163934\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.60      0.55         5\n","           1       0.91      0.91      0.91        47\n","           2       0.88      0.78      0.82         9\n","\n","    accuracy                           0.87        61\n","   macro avg       0.76      0.76      0.76        61\n","weighted avg       0.88      0.87      0.87        61\n","\n","Best Parameters (Random Forest): {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None}\n","Random Forest Accuracy: 0.7049180327868853\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.77      0.87      0.82        47\n","           2       0.40      0.22      0.29         9\n","\n","    accuracy                           0.70        61\n","   macro avg       0.39      0.36      0.37        61\n","weighted avg       0.66      0.70      0.67        61\n","\n","Best Parameters (XGBoost): {'subsample': 0.6, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n","XGBoost Accuracy: 0.7377049180327869\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.91      0.84        47\n","           2       0.67      0.22      0.33         9\n","\n","    accuracy                           0.74        61\n","   macro avg       0.48      0.38      0.39        61\n","weighted avg       0.70      0.74      0.70        61\n","\n","Best Parameters (KNN): {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n","KNN Accuracy: 0.6557377049180327\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.76      0.81      0.78        47\n","           2       0.29      0.22      0.25         9\n","\n","    accuracy                           0.66        61\n","   macro avg       0.35      0.34      0.34        61\n","weighted avg       0.63      0.66      0.64        61\n","\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","import pandas as pd\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# 데이터 파일 경로 설정\n","file_name1 = '/content/drive/MyDrive/total.csv'\n","data = pd.read_csv(file_name1)\n","selected_features = ['Open', 'Close', 'Volume', 'Tech_PCA_1', 'Tech_PCA_2']  # 사용할 독립 변수\n","target_column = 'target'  # 타겟 변수\n","\n","label_map = {-1: 0, 0: 1, 1: 2}\n","data['target'] = data['target'].map(label_map)\n","\n","X = data[selected_features]  # 독립 변수\n","y = data[target_column]      # 타겟 변수\n","\n","# 데이터 분리 (훈련: 80%, 테스트: 20%)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# 데이터 표준화 (로지스틱 회귀와 XGBoost에서 유용)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# 미리 설정한 파라미터 튜플\n","tuned_parameters = {\n","    'logistic_regression': {\n","        'C': 100,\n","        'solver': 'lbfgs'\n","    },\n","    'random_forest': {\n","        'n_estimators': 200,\n","        'max_depth': None,\n","        'min_samples_split': 5,\n","        'min_samples_leaf': 2\n","    },\n","    'xgboost': {\n","        'n_estimators': 200,\n","        'max_depth': 10,\n","        'learning_rate': 0.01,\n","        'subsample': 0.6,\n","        'colsample_bytree': 0.6\n","    },\n","    'knn': {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n","}\n","\n","# 1. 로지스틱 회귀 모델 생성 및 평가\n","logistic_model = LogisticRegression(\n","    C=tuned_parameters['logistic_regression']['C'],\n","    solver=tuned_parameters['logistic_regression']['solver'],\n","    multi_class='multinomial',\n","    random_state=42\n",")\n","logistic_model.fit(X_train, y_train)\n","logistic_preds = logistic_model.predict(X_test)\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logistic_preds))\n","print(classification_report(y_test, logistic_preds))\n","\n","# 2. 랜덤 포레스트 모델 생성 및 평가\n","rf_model = RandomForestClassifier(\n","    n_estimators=tuned_parameters['random_forest']['n_estimators'],\n","    max_depth=tuned_parameters['random_forest']['max_depth'],\n","    min_samples_split=tuned_parameters['random_forest']['min_samples_split'],\n","    min_samples_leaf=tuned_parameters['random_forest']['min_samples_leaf'],\n","    random_state=42\n",")\n","rf_model.fit(X_train, y_train)\n","rf_preds = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n","print(classification_report(y_test, rf_preds))\n","\n","# 3. XGBoost 모델 생성 및 평가\n","xgb_model = XGBClassifier(\n","    n_estimators=tuned_parameters['xgboost']['n_estimators'],\n","    max_depth=tuned_parameters['xgboost']['max_depth'],\n","    learning_rate=tuned_parameters['xgboost']['learning_rate'],\n","    subsample=tuned_parameters['xgboost']['subsample'],\n","    colsample_bytree=tuned_parameters['xgboost']['colsample_bytree'],\n","    objective='multi:softmax',\n","    num_class=len(y.unique()),\n","    random_state=42\n",")\n","xgb_model.fit(X_train, y_train)\n","xgb_preds = xgb_model.predict(X_test)\n","print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n","print(classification_report(y_test, xgb_preds))\n","\n","# 4. KNN 모델 생성 및 평가\n","knn_model = KNeighborsClassifier(\n","    n_neighbors=tuned_parameters['knn']['n_neighbors'],\n","    weights=tuned_parameters['knn']['weights'],\n","    metric=tuned_parameters['knn']['metric']\n",")\n","knn_model.fit(X_train, y_train)\n","knn_preds = knn_model.predict(X_test)\n","print(\"KNN Accuracy:\", accuracy_score(y_test, knn_preds))\n","print(classification_report(y_test, knn_preds))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lS6KUG3j4rxO","executionInfo":{"status":"ok","timestamp":1732296195850,"user_tz":-540,"elapsed":4228,"user":{"displayName":"안태건","userId":"17133080615122350216"}},"outputId":"7e4087af-e984-4b56-b69e-3ed0163ab8f3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Logistic Regression Accuracy: 0.819672131147541\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.40      0.40         5\n","           1       0.88      0.89      0.88        47\n","           2       0.75      0.67      0.71         9\n","\n","    accuracy                           0.82        61\n","   macro avg       0.67      0.65      0.66        61\n","weighted avg       0.82      0.82      0.82        61\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy: 0.7213114754098361\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.89      0.83        47\n","           2       0.67      0.22      0.33         9\n","\n","    accuracy                           0.72        61\n","   macro avg       0.48      0.37      0.39        61\n","weighted avg       0.70      0.72      0.69        61\n","\n","XGBoost Accuracy: 0.7213114754098361\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.78      0.89      0.83        47\n","           2       0.50      0.22      0.31         9\n","\n","    accuracy                           0.72        61\n","   macro avg       0.43      0.37      0.38        61\n","weighted avg       0.67      0.72      0.69        61\n","\n","KNN Accuracy: 0.7049180327868853\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         5\n","           1       0.76      0.89      0.82        47\n","           2       0.50      0.11      0.18         9\n","\n","    accuracy                           0.70        61\n","   macro avg       0.42      0.33      0.34        61\n","weighted avg       0.66      0.70      0.66        61\n","\n"]}]}]}